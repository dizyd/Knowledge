---
title: "Supplement"
subtitle: "For the article #Knowledge: Improving food-related knowledge via seeding implemented as a social media intervention"
date: "`r Sys.Date()`"
format: 
  html:
    code-fold: true
    code-copy: true
    code-tools: true
    df-print: kable
    toc: true
    toc-location: right
    fig-format: svg
    page-layout: full
    fig-height: 4
    fig-width:  9
    fig-align: center
    fig-cap-location: top
    embed-resources: true
  pdf:       
    toc: true
    toc-depth: 2
    fig-height: 4
    fig-width:  9
    fig-align: center
    df-print: kable
    fig-cap-location: top
    execute:
      echo: false
execute:
  warning: false
  messages: false
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| code-summary: "Load packages"

# Packages
library(tidyverse)   # ggplot, dplyr, and friends
library(brms)        # Bayesian modeling through Stan
library(psych)       # For describe()
library(correlation) # For correlations with nicer output
library(patchwork)   # For combining plots
library(parameters)  # Nicer output of model results
library(tidybayes)   # Manipulate brms objects in a tidy way
library(scales)      # For formatting labels in ggplot
library(extrafont)   # to change ggplot font
library(kableExtra)  # for Latextables
library(papaja)      # better printing 
library(bayestestR)  # for describe_posterior() 
library(emmeans)     # for contrast()
library(BayesFactor) # for ttestBF() and anovaBF()
```

```{r ggplot}
#| code-summary: "Define ggplot theme"


# Plot colors
clrs <- c("#54AA8F","#00335B",
          "#22A884FF","#414487FF",
          "#496aa2","#e46c0a","#90b6d4")


# ggplot theme
theme_nice <- function(){
  theme_minimal(base_family = "Jost") +  
    theme(plot.title       = element_text(hjust = 0.5,size = 20),
          panel.grid.minor = element_blank(),
          text             = element_text(size  = 20),
          panel.border     = element_rect(colour = "black", linewidth = 0.5, fill = NA),
          axis.title.x     = element_text(margin = unit(c(3, 0, 0, 0), "mm")),
          axis.title.y     = element_text(margin = unit(c(3, 3, 0, 0), "mm"), angle = 90),
          legend.title     = element_text(face = "bold",size=16),
          strip.text       = element_text(face = "bold"),
          legend.position  = "bottom"
    )}
```

```{r helper-functions}
#| code-summary: "Helper Functions"


brms_plot <- function(mod,lim=c(0,5)){
  

  trace <-   mcmc_plot(temp, type = "trace", variable = "^b_", regex = TRUE) +
              theme_nice() +
              labs(title = "Traceplot")

  pp    <-  pp_check(temp,ndraws = 20) + 
              theme_nice() + xlim(lim[1],lim[2]) +
              labs(title = "Posterior Predictive Check", color = "") +
              scale_color_manual(values = clrs[c(2,7)], labels = c("Observed", "Predicted"))


  return(trace + pp)

  
}


```

```{r data}
#| code-summary: "Load data"
# load data
est <- read_csv2("../Data/df_analysis.csv")
```


The following table shows the list of abbreviations commonly used in the supplement:

```{r}
#| tbl-colwidths: [10,10]

data.frame("Abbreviation" = c("kcal", "CO<sub>2</sub>", "OME", "BF"),
           "Meaning" = c("Kilocalories", "Carbon dioxide", "Order of Magnitude Error", "Bayes Factor")) %>% 
  kbl(escape = F) %>% 
  kable_paper()



```


# Demographics per Condition                                 

```{r demo}
# Make tidy df of needed variables
demo <- est %>% 
          select(ID,trained_criterion,est_criterion,age, gender, education_level) %>% 
          distinct() 



demo %>% 
  distinct() %>% 
  group_by(trained_criterion,est_criterion) %>% 
  summarise(m        = mean(age),
            sd       = sd(age),
            p_female = mean(gender=="female")*100,
            p_male   = mean(gender=="male")*100,
            p_male   = mean(gender=="male")*100,
            p_abi    = mean(education_level=="Abitur")*100,
            p_ba     = mean(education_level=="Bachelor")*100,
            p_ma     = mean(education_level=="Master")*100,
            p_other  = 100-(p_abi+p_ba+p_ma)) %>% 
  mutate_if(is.numeric,printnum,digits=1) %>% 
  mutate(age    = paste0(m," (",sd,")"),
         gender = paste0(p_female," / ",p_male," %"),
         edu    = paste0(p_abi ," / ",p_ba, " / ",p_ma, " / ",p_other," %")) %>% 
  select(trained_criterion,est_criterion,age:edu) %>% 
  kbl(col.names = c("Trained","Estimated","Age","Perc. Female/Male","Perc. GUEQ/BA/MA/Other"),digits = 2) %>% 
  kable_paper() %>% 
  column_spec(1, width="6em") %>% 
  column_spec(2, width="6em") 

```

*Note.* GUEQ = General University Entrance Qualification , BA = Bachelor's degree, MA = Master's degree


# Reactivity Effects in General Criterion Knowledge Question 

```{r bf_reactivity}
# Make tidy df of needed variables
temp <- est %>% 
          select(ID,trained_criterion,est_criterion,Kcal_knowledge,CO2_knowledge) %>% 
          distinct() %>% 
          mutate(trained_f   = factor(trained_criterion),
                 estimated_f = factor(est_criterion)) %>% 
          as.data.frame()  

bf_Kcal = anovaBF(Kcal_knowledge ~ trained_f * estimated_f, data=temp)
bf_CO2  = anovaBF(CO2_knowledge ~ trained_f * estimated_f, data=temp)
```


As stated in the main manuscript, participants reported knowing in general more about the calorie content of food items (*M* = 4.12, *SD* = 1.69) than their CO~2~ footprint (*M* = 2.08, *SD* = 1.32, Î´ = 2.02 \[1.67, 2.36\], BF~10~ \> 1000). However, we also found a small reactivity effect, where participants rated their knowledge of a criterion lower when they had to estimate this criterion beforehand. This effect was found when participants had to estimate calories in the main task (BF~10~ = `r printnum(extractBF(bf_Kcal)$bf[2])`) and also (but to smaller degree) when they had to estimate the carbon footprint (BF~10~ = `r printnum(extractBF(bf_CO2)$bf[2])`). See below for descriptive values and the corresponding figure of individual values.

```{r table_reactivity}
#| code-summary: "Table"

temp %>% 
  group_by(est_criterion) %>% 
  summarize(m_kcal  = mean(Kcal_knowledge),
            sd_kcal = sd(Kcal_knowledge),
            m_CO2   = mean(CO2_knowledge),
            sd_CO2  = sd(CO2_knowledge)) %>% 
  mutate("Knowledge: Kcal" = paste0(printnum(m_kcal)," (",printnum(sd_kcal),")"),
         "Knowledge: CO2" = paste0(printnum(m_CO2)," (",printnum(sd_CO2),")")) %>% 
  select("Estimated Criterion" = est_criterion, `Knowledge: Kcal`, `Knowledge: CO2`) %>% 
  kbl() %>% 
  kable_paper()

```

```{r plots_reactivity}
#| code-summary: "Plot"
#| fig-cap: "Figure S1. General knowledge ratings for C0~2~ footprint and calorie content of food items, depending on the estimated criterion in the main task."

temp %>% 
  pivot_longer(cols      = c(Kcal_knowledge,CO2_knowledge),
               names_to  = "criterion",
               values_to = "values") %>% 
  mutate(criterion   = ifelse(criterion   == "CO2_knowledge", "bold(Knowledge:~CO[2])","bold(Knowledge:~kcal)"),
         estimated_f = ifelse(estimated_f == "CO2","CO[2]","kcal")) %>% 
  ggplot(aes(x = estimated_f, y = values,  group = criterion)) +
    geom_jitter(width=0.1, height = 0.1)+ 
    stat_summary(fun.data = mean_se, geom = "errorbar",aes(color=criterion),width=0.1) +
    stat_summary(fun="mean",geom="line",lwd=0.8,aes(color=criterion)) +
    stat_summary(fun="mean",geom="point",size=4,aes(fill=criterion),shape=21) +
    scale_color_manual(values=c(clrs[4],clrs[3])) +
    scale_fill_manual(values=c(clrs[4],clrs[3])) +
    facet_wrap(. ~ criterion,labeller = label_parsed) +
    theme_nice() +
    scale_x_discrete(labels = parse_format()) +
    scale_y_continuous(breaks = 1:7) +
    labs(x = "Estimated Criterion",
         y = "Knowledge Rating") +
    theme(legend.position = "none") 
```

# Number of Likes Posts                                      

In the preregistration, we also predicted that a greater  seeding  effect when  participants  saw  more posts as indicated by the number of liked posts. However, as already stated in the main text, almost all participants liked every post, see the table below for the descriptive statistics and the Figure S2 for the distribution of liked posts per participant.

```{r table_posts}
#| code-summary: "Table"

est %>% 
  select(ID,"Account"=trained_criterion,n_posts_liked) %>% 
  distinct() %>% 
  group_by(Account) %>% 
  summarise(m   = mean(n_posts_liked),
            sd  = sd(n_posts_liked),
            min = min(n_posts_liked),
            max = max(n_posts_liked)) %>% 
  kbl(col.names = c("Account","M","SD","Min","Max"),digits = 2) %>% 
  kable_paper()

```


```{r plot_dist_posts}
#| code-summary: "Plot"
#| fig-cap: "Figure S2. Distribution of number of liked posts per participant."
#| fig-height: 5
#| fig-width:  8


est %>% 
  select(ID,trained_criterion,est_criterion,n_posts_liked) %>% 
  distinct() %>% 
  mutate(trained_criterion = ifelse(trained_criterion  == "CO2","CO[2]","kcal")) %>% 
  ggplot(aes(x = n_posts_liked,  fill = trained_criterion)) +
    geom_histogram(alpha=0.75,position = "identity",bins=25) +
    scale_fill_manual(values=c(clrs[4],clrs[3]),labels = parse_format()) +
    theme_nice() +
    labs(x    = "Liked X out of 30 possible posts (15 seeding + 15 trivia facts)",
         y    = "Frequency",
         fill = "Account:")

```


In addition, Figure S3 shows the scatter plots with the estimated regression line when using the number of liked posts as an predictor of OME or $\rho$ (all *p*s > 0.05)


```{r plot_posts}
#| code-summary: "Plot"
#| fig-cap: "Figure S3. Relationship of number of liked posts and OME/rank correlation (z-transformed) per trained and estimated criterion"
#| fig-height: 8
#| fig-width:  10

est %>% 
 select(ID,trained_criterion,est_criterion,n_posts_liked, OME_corr, rank_z) %>% 
 group_by(ID,trained_criterion,est_criterion) %>% 
 summarize(OME    = mean(OME_corr),
           rank_z = mean(rank_z),
           n_posts_liked = mean(n_posts_liked),
           .groups="drop") %>% 
  mutate(est_criterion = ifelse(est_criterion == "CO2",
                                "bold(Estimated:~CO[2])",
                                "bold(Estimated:~kcal)"),
         trained_criterion = ifelse(trained_criterion == "CO2",
                                    "CO[2]",
                                    "kcal")) %>% 
  rename(`Estimated Criterion` = est_criterion,
         `Trained Criterion`   = trained_criterion) %>% 
  pivot_longer(cols = c(OME, rank_z), names_to = "DV", values_to = "val") %>% 
  ggplot(aes(x = n_posts_liked, y = val,
             color = `Trained Criterion`, fill = `Trained Criterion`)) +
    geom_point(shape=21,color="black", alpha=.8, size = 2.5) +
    geom_smooth(method="lm",fullrange=TRUE,alpha=0.1) +
    scale_fill_manual(values=c(clrs[4],clrs[3]),labels = parse_format()) +
    scale_color_manual(values=c(clrs[4],clrs[3]),labels = parse_format()) +
    facet_grid(DV~`Estimated Criterion`,labeller = label_parsed) +
    labs(fill  = "Trained Criterion", 
         y     = "Dependet Variable",
         x     = "Number of liked posts") +
    theme_nice()
    



```

# Detailed General Analytical Approach                       

Since the estimation trials are nested in a cross-classified manner within participants and items, we used multi-level modeling to analyze our data. Specifically, we used Bayesian (generalized) mixed-effects models (McElreath, 2020) implemented in the R-package `brms` (BÃ¼rkner, 2017, 2018). This approach allows us to account for the random effects from both, participants and items, and to model the different distributions of the dependent variables in a principled and relatively straightforward manner (Gelman et al., 2014; McElreath, 2020).

Given the skewed exponential distribution inherent to the OME, we used hierarchical log-normal models to analyze all hypotheses with the OME as the dependent variable and hierarchical Gaussian models when the rank-order correlation was the dependent variable. As the log-normal model cannot handle zero values, we replaced the n = 45 (0.5%) trials with an OME of 0 (i.e., when the estimated value matched the actual value) with 0.0001, which is around one order of magnitude below the smallest observed OME of 0.0008[^1]. Additionally, we excluded n = 2 (0.02%) trials where participants gave an estimate of 0 as this would result in an OME of -$\infty$. We used weakly informative priors (Lee & Vanpaemel, 2018) based on previous studies (e.g., BrÃ¶der et al., 2023) for the main predictor variables, and weakly informative priors for the other parameters (e.g., the standard deviation of random effects and the correlation between random effects) to support model convergence and to prevent estimation issues by ensuring priors covered realistic values for the various model parameters. Detailed descriptions of the priors can be found in Appendix B in the main text.

The models were implemented in the `brms`-package (BÃ¼rkner, 2017, 2018) using four Hamiltonian Monte Carlo chains with 1,000 warm-up and 10,000 post-warm-up samples. The convergence of the chains was checked by visual inspection and the standard ËR statistic ($\hat{R}$ < 1.01, Gelman and Rubin, 1992). Hypotheses were tested via model comparison, wherethe full model ($\mathcal{M}_1$) that includes the fixed-effect predictor of interest and all possible random effects was compared to a reference model that only includes the random effects, but not the fixed-effect predictor ($\mathcal{M}_0$, Van Doorn et al., 2023). Thus, although both models assume varying random effects across participants and items, only $\mathcal{M}_1$ assumes an overall $\mathcal{M}_1$ effect of the respective fixed-effects predictor, whereas $\mathcal{M}_0$ assumes that the overall effect is 0 on average (Van Doorn et al., 2023).

We compared the models using Bayes factors (Jeffreys, 1961; Kass & Raftery, 1995; Myung & Pitt, 1997; see Heck et al., 2023 for a review) calculated through bridge sampling (Gronau et al., 2017, 2018). The resulting Bayes factor $BF_{10}$ quantifies the relative evidence for $\mathcal{M}_1$ compared to $\mathcal{M}_0$, indicating how much more probable the data are under $\mathcal{M}_1$ compared to $\mathcal{M}_0$, or framed differently, how much better $\mathcal{M}_1$ is able to predict the observed data than $\mathcal{M}_0$ (Kass & Raftery, 1995; Morey et al., 2016;Vandekerckhove et al., 2015). For example, a Bayes factor of $BF_{10}$ = 5 suggests that the data are five times more likely to occur under M1 than under M0. One convention is that a Bayes factor greater than 3 can be considered as "substantial" evidence and a Bayes factor
greater than 10 as "strong" evidence against the competing model (Wetzels et al., 2011).

We also report the posterior mean and 95% highest density intervals (HDI) of the fixed-effect parameters of interest when the Bayes Factor indicates evidence in favor of $\mathcal{M}_1$ (i.e., $BF_{10}$ > 1, Ly et al., 2020). For the log-normal model, we report the transformed values on the normal response scale for easier interpretation. 



# Seeding Effects on Direct Learning                         


In the analysis of the effects of seeding on calories and CO~2~ reported in the main text, we used only the respective 45 transfer items. Here we report the results when using only the seeding items (see file [`analysis_Hypothesis1_seedingItems.R`](https://github.com/dizyd/Knowledge/blob/main/Scripts/analysis_Hypothesis1_seedingItems.R) for the underlying analysis code).

```{r load_models_seeding}
#| code-summary: "Load models & compute BF"

# Load model files
res_kcal_OME   <- brm(file="../Results/fit_H1a_Kcal_M1_seedingItemsOnly.rds")
res_kcal_OME0  <- brm(file="../Results/fit_H1a_Kcal_M0_seedingItemsOnly.rds")
res_CO2_OME    <- brm(file="../Results/fit_H1a_CO2_M1_seedingItemsOnly.rds")
res_CO2_OME0   <- brm(file="../Results/fit_H1a_CO2_M0_seedingItemsOnly.rds")
res_kcal_rank  <- brm(file="../Results/fit_H1b_kcal_M1_seedingItemsOnly.rds")
res_kcal_rank0 <- brm(file="../Results/fit_H1b_kcal_M0_seedingItemsOnly.rds")
res_CO2_rank   <- brm(file="../Results/fit_H1b_CO2_M1_seedingItemsOnly.rds")
res_CO2_rank0  <- brm(file="../Results/fit_H1b_CO2_M0_seedingItemsOnly.rds")

# Compute BFs
BF_kcal_OME  <- bayes_factor(res_kcal_OME, res_kcal_OME0, silent = T)
BF_CO2_OME   <- bayes_factor(res_CO2_OME, res_CO2_OME0, silent = T)
BF_kcal_rank <- bayes_factor(res_kcal_rank, res_kcal_rank0, silent = T)
BF_CO2_rank  <- bayes_factor(res_CO2_rank, res_CO2_rank0, silent = T)

# remove model objects to save some RAM
rm(list=ls(pattern="res_"))
```


**Metric Knowledge:** We found strong evidence for a large seeding effect on metric knowledge (reduction in OME) on the seeding items for CO~2~ (BF~10~ = `r printnum(BF_CO2_OME$bf)`, *b* = -0.49 [-0.19, -0.82]) and evidence for a smaller seeding effect for calories (BF~10~ = `r printnum(BF_kcal_OME$bf)`, *b* = -0.10 [-0.21, -0.01]). 

**Mapping Knowledge** In contrast, there was weak evidence for an effect of seeding on the mapping knowledge (increase in $\rho$) for participants who estimated calories (BF~10~ = `r printnum(BF_kcal_rank$bf)`, *b* = .08 [.01, .16]) but not CO~2~ (BF~10~ = `r printnum(BF_CO2_rank$bf)`).


# Detailed Modeling Results                                  

Here we provide more detailed modeling results for all models reported in the main manuscript, including a table with the mean, standard deviation, 95\%-HDI, effective sample size (ESS) and $\hat{R}$ for each estimated parameter (random and fixed), as well as figures showing the MCMC-traces for the main fixed effects parameters (intercept and effect parameter) and posterior predictive distributions of the complete model.

## Hypothesis 1a (OME)    


### CO~2~ M0

```{r h1aM0CO2}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H1a_CO2_M0.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()


# Make trace and pp-check plot
brms_plot(temp)
```

### CO~2~ M1

```{r h1aM1CO2}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H1a_CO2_M1.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()

# Make trace and pp-check plot
brms_plot(temp)  + plot_layout(widths = c(2, 1))
```

### kcal M0

```{r h1aM0kcal}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H1a_kcal_M0.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()


# Make trace and pp-check plot
brms_plot(temp)
```

### kcal M1

```{r h1aM1kcal}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H1a_kcal_M1.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()

# Make trace and pp-check plot
brms_plot(temp)  + plot_layout(widths = c(2, 1))
```


    
## Hypothesis 1b ($\rho$) 

### CO~2~ M0

```{r h1bM0CO2}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H1b_CO2_M0.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()


# Make trace and pp-check plot
brms_plot(temp,lim=c(-1,3))
```

### CO~2~ M1

```{r h1bM1CO2}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H1b_CO2_M1.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()

# Make trace and pp-check plot
brms_plot(temp,lim=c(-1,3))  + plot_layout(widths = c(2, 1))
```

### kcal M0

```{r h1bM0kcal}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H1b_kcal_M0.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()


# Make trace and pp-check plot
brms_plot(temp,lim=c(-1,3))
```

### kcal M1

```{r h1bM1kcal}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H1b_kcal_M1.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()

# Make trace and pp-check plot
brms_plot(temp,lim=c(-1,3))  + plot_layout(widths = c(2, 1))
```









    
    
    
    
    



## Hypothesis 2a (OME)    


### CO~2~ M0

```{r h2aM0CO2}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H2a_CO2_M0.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()


# Make trace and pp-check plot
brms_plot(temp)
```

### CO~2~ M1

```{r h2aM1CO2}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H2a_CO2_M1.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()

# Make trace and pp-check plot
brms_plot(temp)  + plot_layout(widths = c(2, 1))
```

### kcal M0

```{r h2aM0kcal}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H2a_kcal_M0.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()


# Make trace and pp-check plot
brms_plot(temp)
```

### kcal M1

```{r h2aM1kcal}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H2a_kcal_M1.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()

# Make trace and pp-check plot
brms_plot(temp)  + plot_layout(widths = c(2, 1))
```



    
## Hypothesis 2b ($\rho$) 

### CO~2~ M0

```{r h2bM0CO2}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H2b_CO2_M0.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()


# Make trace and pp-check plot
brms_plot(temp,lim=c(-1,3))
```

### CO~2~ M1

```{r h2bM1CO2}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H2b_CO2_M1.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()

# Make trace and pp-check plot
brms_plot(temp,lim=c(-1,3))  + plot_layout(widths = c(2, 1))
```

### kcal M0

```{r h2bM0kcal}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H2b_kcal_M0.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()


# Make trace and pp-check plot
brms_plot(temp,lim=c(-1,3))
```

### kcal M1

```{r h2bM1kcal}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H2b_kcal_M1.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()

# Make trace and pp-check plot
brms_plot(temp,lim=c(-1,3))  + plot_layout(widths = c(2, 1))
```









    
    
    
    
    




## Hypothesis 3a (OME)    

### M0


```{r h3aM0}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H3a_M0.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()


# Make trace and pp-check plot
brms_plot(temp) + plot_layout(widths = c(2, 1))
```



### M1


```{r h3aM1}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H3a_M1.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()


# Make trace and pp-check plot
brms_plot(temp) + plot_layout(widths = c(2, 1))
```


### M2


```{r h3aM2}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H3a_M2.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()


# Make trace and pp-check plot
brms_plot(temp) + plot_layout(widths = c(2, 1))
```













## Hypothesis 3b ($\rho$) 


### M0


```{r h3bM0}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H3b_M0.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()


# Make trace and pp-check plot
brms_plot(temp) + plot_layout(widths = c(2, 1))
```



### M1


```{r h3bM1}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H3b_M1.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()


# Make trace and pp-check plot
brms_plot(temp) + plot_layout(widths = c(2, 1))
```


### M2


```{r h3bM2}
#| fig-height: 5
#| fig-width:  14

# Load model 
temp <- brm(file="../Results/fit_H3b_M2.rds")

# Print Formular
temp$formula

# Make model parameter table
model_parameters(temp, centrality = "mean", dispersion = TRUE, 
                 ci_method = "hdi", effects = "all") %>% 
  kable(digits=2) %>% kable_paper()


# Make trace and pp-check plot
brms_plot(temp) + plot_layout(widths = c(2, 1))
```














                                    
                                    
                                    
## Results Analysis

Here we provide the results of the sensitivity analysis (i.e., when using more skeptical and conservative priors) for the tests with an $BF_{10}$ > 1 when using the weakly informative priors.

### Hypothesis 1a (OME) 

For the effect found when estimating calories ($BF_{10}$ = 2.01 in the main text), using the less informative and more skeptical priors did not substantially affect the parameter estimates (as to be expected), but it did affect the Bayes Factor which now indicated no evidence for an effect ($BF_{10}$ = 0.36).

### Hypothesis 2a (OME) 

For the effect found when comparing transfer and seeding items in the calorie criterion ($BF_{10}$ = 1.51 in the main text), using the less informative and more skeptical priors did not substantially affect the parameter estimates, but it did affect the Bayes Factor which now indicated no evidence for an effect ($BF_{10}$ = 0.50).


### Hypothesis 2b ($\rho$) 

For the effect found when comparing transfer and seeding items in the calorie criterion ($BF_{10}$ = 1.39 in the main text), using the less informative and more skeptical priors did not substantially affect the parameter estimates, but it did affect the Bayes Factor which now indicated no evidence for an effect ($BF_{10}$ = 0.69).



# Calculation of Standardized Effect Sizes                   

As suggest by  Westfall et al. ([2014](https://doi.org/10.1037/xge0000014)) and others (e.g., Brysbaert & Stevens, [2018](https://doi.org/10.5334/joc.10)) we calculated standardized effect sizes in terms of Cohens $d$ by dividing the model based effect estimate by the total standard deviation (see file [`analysis_compute_standardized_effect_sizes.R`](https://github.com/dizyd/Knowledge/blob/main/Scripts/analysis_compute_standardized_effect_sizes.R) for the underlying analysis code). However, based on simulations and some checks, we decided against using the model based variance estimate to calculate the total standard deviation and instead directly computed it from the data. The reason for this was that there are some very small OME values as can be seen here when we plot the OME values on a normalized scale which reflects the data used by the log-normal model:


```{r logOME_dist_plot}
#| code-summary: "Plot"
#| fig-cap: "Figure S4. Distribution of normalized OMEs (i.e., log(OMEs))."
#| fig-height: 5
#| fig-width:  8

est %>% 
  filter(item_type == "transfer", est_criterion == "Kcal") %>% # filter data as in Analysis for Hypothesis 1a
   ggplot(aes(x = log(OME_corr))) +
      geom_histogram(alpha=0.75,position = "identity",bins=25) +
      scale_fill_manual(values=c(clrs[4],clrs[3]),labels = parse_format()) +
      theme_nice() +
      labs(x    = "log(OME)",
           y    = "Frequency")

```

These small values (e.g., OME < 0.02, log(OME) < -3.9) have a large biasing effect on the estimated standard deviation coefficients, especially for the random intercepts of items and the residual standard deviation. This increased estimates would then lead to a biased estimate of the effect size. The standard deviations on the normal scale are not influenced that much from these small values, thus we used the estimated total standard deviation from the corresponding OMEs (`sd(OME)`) and the estimated effects on the untransformed scale as reported in the paper to compute the estimated standardized effects. These effects are also more conservative (i.e., smaller) than computing the effects from aggregated data (i.e. from a *t*-test).











# Footnotes

[^1]  In the preregistration, we originally planned to use hurdle log-normal models to account for these 0 values. Upon further consideration, we decided against this approach, since the hurdle model would model the zeros and other non-zero values separately, although both come from the same process, thus biasing the non-zero estimates.